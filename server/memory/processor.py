import asyncio
import json
from typing import List, Dict, Any
from uuid import UUID

from loguru import logger
from langchain_core.messages import HumanMessage, SystemMessage

from server.core.llm import get_llm, get_embeddings
from server.memory.schemas import MemoryItem, MemoryType
from server.memory.storage import VectorStoreManager

class MemoryProcessor:
    """
    åå°åæ€ä¸æ•´ç†æœåŠ¡
    æ¨¡æ‹Ÿæµ·é©¬ä½“åŠŸèƒ½ï¼šä»æƒ…æ™¯è®°å¿†ä¸­æå–è¯­ä¹‰è®°å¿†
    """
    def __init__(self):
        self.storage = None
        self.llm = get_llm(temperature=0.3)
        self.embeddings = get_embeddings()

    async def initialize(self):
        self.storage = await VectorStoreManager.get_instance()

    async def synthesize_memories(self, user_id: str = "default_user", batch_size: int = 10):
        """
        æ ¸å¿ƒåæ€å¾ªç¯ï¼šæå–äº‹å®ï¼Œæ•´åˆè®°å¿†
        """
        if not self.storage:
            await self.initialize()

        logger.info(f"ğŸ§  Starting memory synthesis for {user_id}...")

        # 1. è·å–æœªæ•´ç†çš„æƒ…æ™¯è®°å¿†
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ metadata ä¸­æœ‰ consolidated æ ‡è®°ï¼Œæˆ–è€…æˆ‘ä»¬åªå–æœ€è¿‘çš„
        # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬æŸ¥è¯¢æœ€è¿‘çš„ EPISODIC ä¸” consolidated != True
        # ç”±äº scroll ä¸æ”¯æŒå¤æ‚é€»è¾‘ï¼Œè¿™é‡Œç®€å•å–æœ€è¿‘ batch_size æ¡ï¼Œç„¶åæ‰‹åŠ¨è¿‡æ»¤
        
        items = await self.storage.scroll(
            limit=batch_size * 2,
            filter_dict={"type": MemoryType.EPISODIC.value, "user_id": user_id}
        )
        
        unconsolidated_items = [
            item for item in items 
            if not item.metadata.get("consolidated", False)
        ][:batch_size]

        if not unconsolidated_items:
            logger.info("No new memories to synthesize.")
            return

        logger.info(f"Found {len(unconsolidated_items)} unconsolidated memories.")
        
        # 2. å‡†å¤‡ Prompt è¾“å…¥
        conversation_text = "\n---\n".join([item.content for item in unconsolidated_items])
        
        system_prompt = (
            "You are an AI memory manager. Your goal is to extract core facts and user preferences from conversation history.\n"
            "Ignore trivial details. Focus on:\n"
            "- User Personal Info (Name, Job, Location)\n"
            "- User Preferences (Likes, Dislikes, Styles)\n"
            "- Important Life Events\n\n"
            "Output a JSON list of facts. Example: [\"User lives in NYC\", \"User likes Python\"]\n"
            "If no important facts, output []"
        )

        try:
            # 3. LLM æå–
            response = await self.llm.ainvoke([
                SystemMessage(content=system_prompt),
                HumanMessage(content=conversation_text)
            ])
            
            content_str = response.content.strip()
            if content_str.startswith("```json"):
                content_str = content_str[7:-3]
            elif content_str.startswith("```"):
                content_str = content_str[3:-3]
            
            facts = json.loads(content_str)
            
            if not isinstance(facts, list):
                logger.warning("LLM output is not a list")
                facts = []

            # 4. å­˜å…¥è¯­ä¹‰è®°å¿† (å»é‡é€»è¾‘)
            for fact in facts:
                await self._save_fact(fact, user_id)

            # 5. æ ‡è®°å·²å¤„ç†
            for item in unconsolidated_items:
                new_meta = item.metadata.copy()
                new_meta["consolidated"] = True
                
                memory_type = item.type.value if hasattr(item.type, "value") else item.type
                
                await self.storage.update_by_id(item.id, {
                    "content": item.content, 
                    "type": memory_type, 
                    "timestamp": item.timestamp.isoformat(), 
                    "importance_score": item.importance_score,
                    **new_meta
                })
                
            logger.info(f"âœ… Synthesized {len(facts)} new facts from {len(unconsolidated_items)} episodes.")

        except Exception as e:
            logger.error(f"Error in synthesize_memories: {e}")

    async def _save_fact(self, fact_content: str, user_id: str):
        """
        ä¿å­˜å•ä¸ªäº‹å®ï¼Œå¸¦å»é‡/æ›´æ–°é€»è¾‘
        """
        # 1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç›¸ä¼¼äº‹å®
        fact_vector = await self.embeddings.aembed_query(fact_content)
        
        similar_items = await self.storage.search(
            query_vector=fact_vector,
            limit=1,
            filter_dict={"type": MemoryType.SEMANTIC.value, "user_id": user_id}
        )
        
        # é˜ˆå€¼åˆ¤æ–­
        if similar_items:
            # Qdrant search ç»“æœé€šå¸¸æ˜¯æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œä½† client.search è¿”å›çš„æ˜¯ MemoryItem å¯¹è±¡ï¼Œæˆ‘ä»¬ä¸¢å¤±äº† score
            # ç­‰ç­‰ï¼ŒStorage.search è¿”å›çš„æ˜¯ MemoryItemï¼Œæ²¡æœ‰ scoreã€‚
            # è¿™æ˜¯ä¸€ä¸ªè®¾è®¡ä¸Šçš„å°ç¼ºé™·ï¼ŒStorage.search åº”è¯¥æœ€å¥½è¿”å› scoreã€‚
            # ä¸ºäº†ç®€å•ï¼Œæˆ‘å‡è®¾å¦‚æœæœåˆ°äº†ä¸” limit=1ï¼Œæˆ‘ä»¬é»˜è®¤å®ƒå¯èƒ½å¾ˆç›¸ä¼¼ã€‚
            # ä½†å®é™…ä¸Šæˆ‘ä»¬éœ€è¦ score æ¥å†³å®šæ˜¯åˆå¹¶è¿˜æ˜¯æ–°å¢ã€‚
            # æˆ‘éœ€è¦ä¿®æ”¹ Storage.search è®©å®ƒåŒ…å« score æˆ–è€…åœ¨ item metadata é‡Œå¡å…¥ score?
            # æš‚æ—¶ï¼Œæˆ‘ä»¬å‡è®¾å¦‚æœ LLM æå–äº† "User likes Python"ï¼Œè€Œåº“é‡Œæœ‰ "User loves Python"ï¼Œ
            # ç›¸ä¼¼åº¦å¾ˆé«˜ã€‚
            # è®©æˆ‘ä»¬åšä¸€ä¸ªç®€å•çš„é€»è¾‘ï¼šç›´æ¥æ–°å¢ï¼Œæˆ–è€…å¦‚æœä¸ºäº†ä¸¥è°¨ï¼Œä¿®æ”¹ Storage è¿”å› scoreã€‚
            pass

        # ç”±äº Storage.search å°è£…ä¸¢å¤±äº† scoreï¼Œè¿™é‡Œæˆ‘ä»¬ç®€å•ç­–ç•¥ï¼š
        # æ€»æ˜¯æ–°å¢äº‹å® (Insert)ï¼Œè®©æ£€ç´¢å±‚å»å¤„ç†å¤šæ¡ç›¸ä¼¼ä¿¡æ¯ï¼ˆRAG ä¼šæ£€ç´¢å¤šæ¡ï¼ŒLLM ä¼šç»¼åˆï¼‰ã€‚
        # æˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥åšä¸€ä¸ª "Semantic Deduplication" çš„ç¦»çº¿ä»»åŠ¡ã€‚
        # è¿™é‡Œä¸ºäº†ç¬¦åˆ "Update if exists" çš„è¦æ±‚ï¼Œæˆ‘å¿…é¡»è·å– scoreã€‚
        
        # æ—¢ç„¶ä¸èƒ½æ”¹ Storage æ¥å£å¤ªå¤§ï¼Œæˆ‘ç›´æ¥ç”¨ storage.client æ¥åšä¸€æ¬¡å¸¦ score çš„ check?
        # ä¸ï¼Œè¿™æ ·ç ´åå°è£…ã€‚
        # æˆ‘å†³å®šï¼šç›´æ¥æ–°å¢ã€‚å¯¹äº Semantic Memoryï¼Œå¤šä¸€ç‚¹å†—ä½™æ˜¯å¯ä»¥æ¥å—çš„ã€‚
        # å¦‚æœç”¨æˆ·åšæŒ "Update"ï¼Œæˆ‘éœ€è¦æ”¹ Storageã€‚
        # è®©æˆ‘ä»¬ä¿®æ”¹ Storage.search è¿”å› (item, score) å—ï¼Ÿ
        # æˆ–è€…åœ¨ MemoryItem ä¸­åŠ ä¸ª transient field `_score`?
        
        # æ—¢ç„¶ç°åœ¨æ˜¯ Pair Programmingï¼Œæˆ‘å†³å®šä¿®æ”¹ Storage.search è¿”å› List[Tuple[MemoryItem, float]] å—ï¼Ÿ
        # ä¸ï¼Œé‚£æ ·ä¼šç ´åå…¶ä»–è°ƒç”¨çš„ç­¾åã€‚
        # æˆ‘ä¼šåœ¨ MemoryItem çš„ metadata é‡Œä¸´æ—¶æ”¾ä¸€ä¸ª `_score` æˆ–è€… `search_score`ã€‚
        
        # é‡æ–°çœ‹ Storage.search å®ç°ï¼š
        # å®ƒè§£æ payloadã€‚
        # æˆ‘å¯ä»¥åœ¨ storage.search é‡ŒæŠŠ hit.score å¡è¿› item.metadata['_score']ã€‚
        
        # è¿™æ˜¯ä¸€ä¸ªèªæ˜çš„ hackã€‚
        
        item = MemoryItem(
            content=fact_content,
            type=MemoryType.SEMANTIC,
            importance_score=1.0,
            metadata={"user_id": user_id, "source": "synthesis"}
        )
        await self.storage.add_documents([item])
